"""Training script for Q-learning agent."""

import os
import sys
import threading
import random
from pathlib import Path
from typing import Optional, Literal, Dict

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import tyro

from src.envs import Connect4Env, RewardConfig
from src.agents import QLearningAgent
from src.selfplay import OpponentPool
from src.utils import MetricsLogger
from src.training.random_opening import RandomOpeningConfig, maybe_apply_random_opening


def train_qlearning(
    num_episodes: int = 10000,
    learning_rate: float = 0.1,
    discount_factor: float = 0.99,
    epsilon: float = 0.1,
    epsilon_decay: float = 0.995,
    epsilon_min: float = 0.01,
    eval_freq: int = 100,
    eval_episodes: int = 100,
    save_freq: int = 1000,
    checkpoint_dir: str = "data/checkpoints",
    log_dir: str = "data/logs",
    seed: int = 42,
    stop_flag: Optional[threading.Event] = None,
    reward_config: Optional[RewardConfig] = None,
    heuristic_distribution: Optional[Dict[str, float]] = None,
    random_opening_config: Optional[RandomOpeningConfig] = None,
):
    """
    Train Q-learning agent using self-play.
    
    Args:
        num_episodes: Number of training episodes
        learning_rate: Learning rate
        discount_factor: Discount factor
        epsilon: Initial epsilon
        epsilon_decay: Epsilon decay rate
        epsilon_min: Minimum epsilon
        eval_freq: Evaluation frequency
        eval_episodes: Number of episodes for evaluation
        save_freq: Checkpoint save frequency
        checkpoint_dir: Directory for checkpoints
        log_dir: Directory for logs
        seed: Random seed
        reward_config: Reward configuration (default: RewardConfig with default values)
        heuristic_distribution: Distribution of heuristic opponents (default: uniform distribution)
        random_opening_config: Configuration for randomized opening prologues (optional)
    """
    # Create directories
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    # Initialize reward config
    if reward_config is None:
        reward_config = RewardConfig()
    
    # Initialize heuristic distribution
    if heuristic_distribution is None:
        # Default: uniform distribution
        heuristic_distribution = {
            "random": 1.0 / 3.0,
            "heuristic": 1.0 / 3.0,
            "smart_heuristic": 1.0 / 3.0,
        }
    
    # Initialize opponent pool
    opponent_pool = OpponentPool(
        device=None,  # Q-learning doesn't use GPU
        seed=seed,
        self_play_config=None,  # Q-learning doesn't support self-play
        heuristic_distribution=heuristic_distribution,
    )
    
    # Initialize environment with reward configuration
    env = Connect4Env(
        rows=6, 
        cols=7,
        reward_config=reward_config,
    )
    
    # Initialize agents
    learning_agent = QLearningAgent(
        learning_rate=learning_rate,
        discount_factor=discount_factor,
        epsilon=epsilon,
        epsilon_decay=epsilon_decay,
        epsilon_min=epsilon_min,
        seed=seed,
    )
    
    # Initialize logger
    logger = MetricsLogger(log_dir=log_dir)
    
    print("Starting Q-learning training...")
    print(f"Episodes: {num_episodes}")
    print(f"Learning rate: {learning_rate}")
    print(f"Discount factor: {discount_factor}")
    print(f"Initial epsilon: {epsilon}")
    print(f"Heuristic distribution: {heuristic_distribution}")
    if random_opening_config is not None:
        print(
            "Random opening: "
            f"p={random_opening_config.probability:.2f}, "
            f"half-moves={random_opening_config.min_half_moves}-{random_opening_config.max_half_moves}"
        )
    print()
    
    training_completed = False
    for episode in range(num_episodes):
        # Check stop flag
        if stop_flag is not None and stop_flag.is_set():
            print(f"\nüõë –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –Ω–∞ —ç–ø–∏–∑–æ–¥–µ {episode + 1}/{num_episodes}")
            # Save checkpoint before stopping
            checkpoint_path = os.path.join(checkpoint_dir, f"qlearning_episode_{episode + 1}_stopped.pkl")
            learning_agent.save(checkpoint_path)
            print(f"Checkpoint saved: {checkpoint_path}")
            break
        
        opponent = opponent_pool.sample_opponent(episode)
        
        obs = env.reset()
        obs, _, prologue_done = maybe_apply_random_opening(
            env=env,
            initial_obs=obs,
            config=random_opening_config,
            rng=random,
        )
        if prologue_done:
            # –†–µ–¥–∫–∏–π —Å–ª—É—á–∞–π: —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–ª–æ–≥ –∑–∞–≤–µ—Ä—à–∏–ª –∏–≥—Ä—É. –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–Ω–æ–≤–æ –±–µ–∑ –ø—Ä–æ–ª–æ–≥–∞.
            obs = env.reset()
        done = False
        current_player = 0  # 0: learning_agent, 1: opponent
        episode_reward = 0
        episode_length = 0
        
        # Pending transition: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –∞–≥–µ–Ω—Ç–∞ –¥–æ —Ö–æ–¥–∞ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞
        pending_obs = None
        pending_action = None
        pending_reward = 0.0
        
        while not done:
            legal_actions = env.get_legal_actions()
            
            if current_player == 0:
                # –•–æ–¥ –∞–≥–µ–Ω—Ç–∞
                action = learning_agent.select_action(obs, legal_actions)
                next_obs, reward, done, info = env.step(action)
                
                if done:
                    # –ê–≥–µ–Ω—Ç –≤—ã–∏–≥—Ä–∞–ª –∏–ª–∏ –Ω–∏—á—å—è –ø–æ—Å–ª–µ –µ–≥–æ —Ö–æ–¥–∞
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º transition —Å—Ä–∞–∑—É —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ä–µ–≤–∞—Ä–¥–æ–º
                    final_reward = reward  # env —É–∂–µ –¥–∞–ª reward_win –∏–ª–∏ reward_draw
                    learning_agent.observe((obs, action, final_reward, next_obs, True, info))
                    episode_reward += final_reward
                else:
                    # –ò–≥—Ä–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –¥–æ —Ö–æ–¥–∞ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞
                    pending_obs = obs
                    pending_action = action
                    pending_reward = reward  # shaping reward (—Ç—Ä–æ–π–∫–∏ –∏ —Ç.–ø.)
                    # –ù–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º observe –ø–æ–∫–∞ —Å–æ–ø–µ—Ä–Ω–∏–∫ –Ω–µ —Å—Ö–æ–¥–∏–ª
                
                obs = next_obs
            else:
                # –•–æ–¥ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞
                action = opponent.select_action(obs, legal_actions)
                next_obs, opp_reward, done, info = env.step(action)
                # opp_reward - –¥–ª—è —Å–æ–ø–µ—Ä–Ω–∏–∫–∞, –Ω–∞–º –Ω–µ –Ω—É–∂–µ–Ω
                
                if pending_obs is not None:
                    # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –∞–≥–µ–Ω—Ç–∞
                    if done:
                        # –°–æ–ø–µ—Ä–Ω–∏–∫ –∑–∞–≤–µ—Ä—à–∏–ª –∏–≥—Ä—É
                        winner = info.get("winner")
                        if winner == 0:
                            # –ù–∏—á—å—è
                            final_reward = reward_config.draw
                        else:
                            # –°–æ–ø–µ—Ä–Ω–∏–∫ –ø–æ–±–µ–¥–∏–ª ‚Üí –∞–≥–µ–Ω—Ç –ø—Ä–æ–∏–≥—Ä–∞–ª
                            final_reward = reward_config.loss  # –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–≤–∞—Ä–¥
                        
                        learning_agent.observe((
                            pending_obs, pending_action, final_reward, next_obs, True, info
                        ))
                        episode_reward += final_reward
                    else:
                        # –ò–≥—Ä–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ —Ö–æ–¥–∞ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞
                        final_reward = pending_reward  # shaping reward –∑–∞ —Ö–æ–¥ –∞–≥–µ–Ω—Ç–∞
                        learning_agent.observe((
                            pending_obs, pending_action, final_reward, next_obs, False, info
                        ))
                        episode_reward += final_reward
                    
                    # –û—á–∏—â–∞–µ–º pending transition
                    pending_obs = None
                    pending_action = None
                    pending_reward = 0.0
                
                obs = next_obs
            
            current_player = 1 - current_player
            episode_length += 1
        
        # Log metrics
        logger.log("episode_reward", episode_reward, step=episode)
        logger.log("episode_length", episode_length, step=episode)
        logger.log("epsilon", learning_agent.epsilon, step=episode)
        
        # Decay epsilon once per episode (not per step!)
        if learning_agent.training and learning_agent.epsilon > learning_agent.epsilon_min:
            learning_agent.epsilon *= learning_agent.epsilon_decay
        
        logger.increment_episode()
        
        # Evaluation
        if (episode + 1) % eval_freq == 0:
            win_rate, draw_rate, loss_rate = evaluate_agent(
                learning_agent, 
                opponent_pool, 
                episode,
                eval_episodes, 
                reward_config=reward_config,
                seed=seed,
            )
            logger.log_dict({
                "win_rate": win_rate,
                "draw_rate": draw_rate,
                "loss_rate": loss_rate,
            }, step=episode)
            
            print(
                f"Episode {episode + 1}/{num_episodes} | "
                f"Win rate: {win_rate:.2%} | "
                f"Draw rate: {draw_rate:.2%} | "
                f"Loss rate: {loss_rate:.2%} | "
                f"Epsilon: {learning_agent.epsilon:.4f}"
            )
        
        # Save checkpoint
        if (episode + 1) % save_freq == 0:
            checkpoint_path = os.path.join(checkpoint_dir, f"qlearning_episode_{episode + 1}.pkl")
            learning_agent.save(checkpoint_path)
            print(f"Checkpoint saved: {checkpoint_path}")
    
    # Mark training as completed if we reached the end
    if episode == num_episodes - 1:
        training_completed = True
    
    # Final save - always save final checkpoint, even if stopped early
    final_path = os.path.join(checkpoint_dir, "qlearning_final.pkl")
    learning_agent.save(final_path)
    if training_completed:
        print(f"\nTraining completed! Final model saved: {final_path}")
    else:
        print(f"\nTraining stopped. Final model saved: {final_path}")
    
    logger.close()


def evaluate_agent(
    agent, 
    opponent_pool, 
    episode: int,
    num_episodes: int, 
    reward_config: RewardConfig,
    seed: Optional[int] = None,
) -> tuple:
    """
    Evaluate agent against opponents sampled from opponent pool.
    
    Args:
        agent: Agent to evaluate
        opponent_pool: Opponent pool to sample opponents from
        episode: Current episode number (for opponent sampling)
        num_episodes: Number of evaluation episodes
        reward_config: Reward configuration
        seed: Random seed
        
    Returns:
        Tuple of (win_rate, draw_rate, loss_rate)
    """
    env = Connect4Env(
        rows=6, 
        cols=7,
        reward_config=reward_config,
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ epsilon –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0 –¥–ª—è –∂–∞–¥–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ
    old_epsilon = agent.epsilon
    agent.epsilon = 0.0
    agent.eval()
    wins = 0
    draws = 0
    losses = 0
    
    if seed is not None:
        random.seed(seed)
    
    for episode_idx in range(num_episodes):
        opponent = opponent_pool.sample_opponent(episode + episode_idx)
        
        obs = env.reset()
        done = False
        # –†–∞–Ω–¥–æ–º–∏–∑–∏—Ä—É–µ–º, –∫—Ç–æ —Ö–æ–¥–∏—Ç –ø–µ—Ä–≤—ã–º (–∫–∞–∫ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ)
        agent_goes_first = random.random() < 0.5
        agent_is_player_1 = agent_goes_first  # –ï—Å–ª–∏ –∞–≥–µ–Ω—Ç —Ö–æ–¥–∏—Ç –ø–µ—Ä–≤—ã–º, –æ–Ω player 1
        
        while not done:
            legal_actions = env.get_legal_actions()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫—Ç–æ –¥–æ–ª–∂–µ–Ω —Ö–æ–¥–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ env.current_player
            # env.current_player == 1 –æ–∑–Ω–∞—á–∞–µ—Ç player 1
            # env.current_player == -1 –æ–∑–Ω–∞—á–∞–µ—Ç player -1
            if env.current_player == 1:
                # –•–æ–¥–∏—Ç player 1
                if agent_is_player_1:
                    action = agent.select_action(obs, legal_actions)
                else:
                    action = opponent.select_action(obs, legal_actions)
            else:
                # –•–æ–¥–∏—Ç player -1
                if agent_is_player_1:
                    action = opponent.select_action(obs, legal_actions)
                else:
                    action = agent.select_action(obs, legal_actions)
            
            next_obs, reward, done, info = env.step(action)
            
            if done:
                winner = info.get("winner")
                # winner == 1 –æ–∑–Ω–∞—á–∞–µ—Ç player 1 –≤—ã–∏–≥—Ä–∞–ª
                # winner == -1 –æ–∑–Ω–∞—á–∞–µ—Ç player -1 –≤—ã–∏–≥—Ä–∞–ª
                # winner == 0 –æ–∑–Ω–∞—á–∞–µ—Ç –Ω–∏—á—å—é
                if winner == 0:
                    draws += 1
                elif (winner == 1 and agent_is_player_1) or (winner == -1 and not agent_is_player_1):
                    # –ê–≥–µ–Ω—Ç –≤—ã–∏–≥—Ä–∞–ª
                    wins += 1
                else:
                    # –°–æ–ø–µ—Ä–Ω–∏–∫ –≤—ã–∏–≥—Ä–∞–ª
                    losses += 1
                break
            
            obs = next_obs
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ epsilon –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –∞–≥–µ–Ω—Ç–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    agent.epsilon = old_epsilon
    agent.train()
    
    win_rate = wins / num_episodes
    draw_rate = draws / num_episodes
    loss_rate = losses / num_episodes
    
    return win_rate, draw_rate, loss_rate


if __name__ == "__main__":
    tyro.cli(train_qlearning)

