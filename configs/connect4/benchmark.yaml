# Self-play with Quantile Regression DQN (distributional RL)
# Optionally enables predicting return distribution instead of scalar Q
seed: 42

game:
  id: connect4
  params:
    rows: 6
    cols: 7

agent:
  id: dqn
  params:
    dueling: false
    use_distributional: false
    n_quantiles: 32
    learning_rate: 1.0e-4
    discount_factor: 0.99
    use_twin_q: false
    epsilon: 1.0
    epsilon_decay: 0.99998
    epsilon_min: 0.05
    batch_size: 64
    replay_buffer_size: 1000000
    use_per: false
    target_update_freq: 1000
    soft_update: false
    tau: 0.001
    n_step: 1
    device: cuda:0
    metrics_interval: 100
    update_every: 16
    updates_per_step: 4
    grad_clip_norm: 2
    target_q_clip: 2.0
    value_reg_weight: 1.0e-4

train:
  total_steps: 2000
  deterministic: false
  track_timings: true
  apply_augmentation: false
  start_policy: random
  random_opening:
    probability: 0.5
    min_half_moves: 1
    max_half_moves: 6
  opponent_sampler:
    type: pool
    params:
      heuristic_distribution:
        random: 0.2
        heuristic: 0.8
      self_play:
        start_episode: 100000
        current_self_fraction: 0.6
        past_self_fraction: 0.4
        max_frozen_agents: 20
        save_every: 10000
  callbacks:
    - type: checkpoint
      params:
        interval: 10000
        prefix: dqn
    - type: metrics_file
      params:
        interval: 100
    - type: epsilon_decay
      params:
        every: step
    - type: lr_decay
      params:
        interval_steps: 50000
        decay_factor: 0.9
        min_lr: 1.0e-6
        optimizer_attr: optimizer
        metric_key: learning_rate

eval:
  enabled: false
  num_episodes: 50
  deterministic: true
